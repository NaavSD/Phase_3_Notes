{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e81bba0f",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e2baa0",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9436c7d",
   "metadata": {},
   "source": [
    "EDA -> Dummy classifier -> Evaluate dummy model -> N model -> Compare and select models\n",
    "\n",
    "**EDA**<br>\n",
    "EDA things\n",
    "\n",
    "**Dummy Classifier**<br>\n",
    "Create a dummy classifier and run a dummy model\n",
    "\n",
    "**Evaluate dummy model**<br>\n",
    "Evaluate the dummy model using relavent evaluations.\n",
    "\n",
    "**N model**<br>\n",
    "If data engineering and exploration is required for the N model, you would do that now.<br>\n",
    "Once data is set for your second model, you would run and evaluate that model.\n",
    "\n",
    "The process repeats for each N model after the dummy model.\n",
    "\n",
    "**Compare and select**<br>\n",
    "At the end, you would evaluate and select between your models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00452fd",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b44e2",
   "metadata": {},
   "source": [
    "- To regularize data, they must be standard scaled first\n",
    "- Regularize data to better fit model (fix under/over fit)\n",
    "- Must use Test/Train split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816042b",
   "metadata": {},
   "source": [
    "**Ridge**\n",
    "\n",
    "Start with this type of regularization.<br>\n",
    "Gentler. More useful for smaller data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718f061",
   "metadata": {},
   "source": [
    "**Lasso**\n",
    "\n",
    "Rough regularization. Drops large coefficients. Useful for datasets with high volume of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974f4ba",
   "metadata": {},
   "source": [
    "**Elastic Net**\n",
    "\n",
    "Combination of Lasso and Ridge. % of each usually set standard by employer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ff09d",
   "metadata": {},
   "source": [
    "## Missing Vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5750456",
   "metadata": {},
   "source": [
    "The `MissingIndicator` from `sklearn` will mark the missing values in an input array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e791342e",
   "metadata": {},
   "source": [
    "**Numeric Variables**\n",
    "\n",
    "`SimpleImputer` can impute numeric or categorical vars as selected by `strategy=''`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b155e5",
   "metadata": {},
   "source": [
    "**Categorical Variables**\n",
    "\n",
    "`SimpleImputer` can be used for Categorical vars as designated by `strategy=''` or `fill_value=`\n",
    "\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bea7f",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac254ef",
   "metadata": {},
   "source": [
    "### Dummy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356c862",
   "metadata": {},
   "source": [
    "Dummy model for Logistic Regression is called a classifier.\n",
    "\n",
    "Use `dummy_model = DummyClassifier(strategy=\"most_frequent\")` to instantiate the dummy model for a logistic regression.<br>\n",
    "Fit to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298f8c08",
   "metadata": {},
   "source": [
    "### Measures of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5862c",
   "metadata": {},
   "source": [
    "**Accuracy**\n",
    "\n",
    "See **F1**<br>\n",
    "Cannot use with imbalanced data.\n",
    "\n",
    "Balancing data will be covered later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddffe52",
   "metadata": {},
   "source": [
    "**Log-loss**\n",
    "\n",
    "Can be found using `log_loss(y_test, y_hat_hd)`<br>\n",
    "Where `y_hat_hd` is your predicted y's from your X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de103064",
   "metadata": {},
   "source": [
    "**F1 (Equal)**\n",
    "\n",
    "$\\frac{TP + TN}{TN + TP + FN + FP}$\n",
    "\n",
    "Takes into account false negatives and false positives equally.\n",
    "\n",
    "Higher percentage is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fba01f",
   "metadata": {},
   "source": [
    "**Precision**\n",
    "\n",
    "$\\frac{TP}{TP + FP}$\n",
    "\n",
    "Captures False Positives. Useful for situations where false positives are considered more important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420f3d3",
   "metadata": {},
   "source": [
    "**Recall**\n",
    "\n",
    "$\\frac{TP}{TP + FN}$\n",
    "\n",
    "Captures False Negatives. Useful for situations where false negatives are considered more important<br>\n",
    "This is a **True Positive Rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7586dfb",
   "metadata": {},
   "source": [
    "**False Positive Rate (FPR)**\n",
    "\n",
    "$\\frac{FP}{FP + TN}$\n",
    "\n",
    "Measures how many of the negative casses we incorrectly classified as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e9941",
   "metadata": {},
   "source": [
    "**AUC**\n",
    "\n",
    "Model comparison metric<br>\n",
    "Larger Area Under Curve (AUC) is better.\n",
    "\n",
    "Can be found using `roc_auc_score(y_test, y_hat_hd)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba572298",
   "metadata": {},
   "source": [
    "**ROC**\n",
    "\n",
    "Receiver Operating Characteristic (ROC) Curve<br>\n",
    "plots the true-positive rate vs. the false-positive rate.\n",
    "\n",
    "Can be plotted manually with:\n",
    "```\n",
    "# Extract the probability predictions for the \"1\" class (heart disease)\n",
    "y_hat_hd = y_prob[:, 1]\n",
    "\n",
    "# Get the FPR and TPR data\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_hat_hd)\n",
    "\n",
    "# Plot the FPR and TPR data\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr)\n",
    "ax.plot((0,1), (0,1), 'k--');\n",
    "```\n",
    "\n",
    "or using `plot_roc_curve(hd_model, X_test_sc, y_test);` <br>\n",
    "Where `X_test_sc` is your scaled X values in your test set and `hd_model` is your logistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb8175",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796e901",
   "metadata": {},
   "source": [
    "Can be used to calculate **F1**, **Precision** and **Recall**.\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "TN & FP \\\\\n",
    "FN & TP\n",
    "\\end{bmatrix}$\n",
    "\n",
    "After running models, you can use `confusion_matrix(y_test, y_pred)` to generate a matrix array and `plot_confusion_matrix(cred_model, X_test_sc, y_test);` to visualize the matrix.\n",
    "\n",
    "Use this block to parse your confusion matrix into TN, TP, FN, FP. Where cm_1 is your confusion matrix array.\n",
    "```\n",
    "tn = cm_1[0, 0]\n",
    "fp = cm_1[0, 1]\n",
    "fn = cm_1[1, 0]\n",
    "tp = cm_1[1, 1]\n",
    "```\n",
    "These can then be used to write into the measures of fit formulas above.\n",
    "\n",
    "These can also be displayed using `classification_report(y_test, y_pred)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4067e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
